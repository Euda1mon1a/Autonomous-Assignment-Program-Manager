# =============================================================================
# Residency Scheduler - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control
# =============================================================================

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# PostgreSQL password for the scheduler user
# Use a strong, unique password in production
DB_PASSWORD=your_secure_database_password_here

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------
# Secret key for JWT token signing
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
# CRITICAL: Use a unique, random value in production
SECRET_KEY=your_secret_key_here_generate_a_random_64_char_string

# Webhook secret for validating incoming webhook requests
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
# CRITICAL: Use a unique, random value in production (minimum 32 characters)
WEBHOOK_SECRET=your_webhook_secret_here_generate_a_random_64_char_string

# Redis password for authentication
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# For development: defaults to 'dev_only_password' if not set
# For production: MUST be set to a strong random value
REDIS_PASSWORD=your_redis_password_here_generate_a_random_string

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------
# Debug mode - set to false in production
# Enables detailed error messages and hot reload in development
DEBUG=true

# -----------------------------------------------------------------------------
# gRPC Configuration (optional)
# -----------------------------------------------------------------------------
# Port for gRPC server (if enabled)
# Default: 50051 (standard gRPC port)
GRPC_PORT=50051

# Maximum number of concurrent gRPC workers
# Increase for high-traffic deployments
# Default: 10
GRPC_MAX_WORKERS=10

# Enable TLS/SSL for gRPC connections
# Production: true (requires valid certificates)
# Development: false (unencrypted connections)
GRPC_ENABLE_TLS=false

# Path to TLS certificate file (required if GRPC_ENABLE_TLS=true)
# Example: /etc/ssl/certs/grpc-server.pem
# GRPC_TLS_CERT_PATH=/path/to/cert.pem

# Path to TLS private key file (required if GRPC_ENABLE_TLS=true)
# Example: /etc/ssl/private/grpc-server-key.pem
# GRPC_TLS_KEY_PATH=/path/to/key.pem

# -----------------------------------------------------------------------------
# Error Reporting Configuration (optional)
# -----------------------------------------------------------------------------
# Sentry DSN for error tracking and monitoring
# Sign up at https://sentry.io/ to get your DSN
# Leave empty to disable Sentry error reporting
# Format: https://<key>@sentry.io/<project_id>
# SENTRY_DSN=https://examplePublicKey@o0.ingest.sentry.io/0

# -----------------------------------------------------------------------------
# Single Sign-On (SSO) Configuration (optional)
# -----------------------------------------------------------------------------
# Enable SSO authentication (requires additional configuration)
# Default: false (use local authentication)
# SSO_ENABLED=false

# Enable SAML 2.0 authentication
# Requires SSO_ENABLED=true and SAML identity provider setup
# SSO_SAML_ENABLED=false

# Enable OAuth2/OIDC authentication
# Requires SSO_ENABLED=true and OAuth provider configuration
# Examples: Azure AD, Okta, Google Workspace
# SSO_OAUTH2_ENABLED=false

# -----------------------------------------------------------------------------
# Notification Email Configuration (optional)
# -----------------------------------------------------------------------------
# Email addresses for compliance violation notifications
# Format: comma-separated list of email addresses
# Example: compliance@hospital.mil,quality@hospital.mil
# COMPLIANCE_STAKEHOLDER_EMAILS=compliance@example.com

# Email addresses for executive reports and critical alerts
# Format: comma-separated list of email addresses
# Example: cmo@hospital.mil,chief-of-staff@hospital.mil
# EXECUTIVE_STAKEHOLDER_EMAILS=exec@example.com

# -----------------------------------------------------------------------------
# CORS Configuration
# -----------------------------------------------------------------------------
# Allowed origins for cross-origin requests
# In production, set to your actual frontend domain
# Format: JSON array of strings
CORS_ORIGINS=["http://localhost:3000"]

# -----------------------------------------------------------------------------
# Frontend Configuration
# -----------------------------------------------------------------------------
# Backend API URL that the frontend will use
# In production, use your actual backend domain/IP
NEXT_PUBLIC_API_URL=http://localhost:8000

# -----------------------------------------------------------------------------
# MCP Server Configuration
# -----------------------------------------------------------------------------
# Log level for MCP server (DEBUG, INFO, WARNING, ERROR)
# DEBUG shows all tool invocations and API calls
MCP_LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# N8N Workflow Automation Configuration
# -----------------------------------------------------------------------------
# N8N basic authentication credentials
# SECURITY: Change these defaults in production!
# Generate password with: python -c "import secrets; print(secrets.token_urlsafe(32))"
N8N_USER=admin
N8N_PASSWORD=CHANGE_ME_IN_PRODUCTION

# N8N host and webhook URL
# In production, set to your actual domain
N8N_HOST=localhost
N8N_WEBHOOK_URL=http://localhost:5678

# =============================================================================
# OpenTelemetry / Distributed Tracing Configuration
# =============================================================================
# Provides distributed tracing across backend services (API, workers, scheduler)
# Useful for debugging latency issues, tracking request flows, and monitoring dependencies
# Default: TELEMETRY_ENABLED=false (disabled for development to avoid performance impact)
#
# PRODUCTION SETUP:
# 1. Enable tracing: TELEMETRY_ENABLED=true
# 2. Set backend endpoint (where traces are exported):
#    - Jaeger all-in-one: http://jaeger:16686 (or your host:port)
#    - OpenTelemetry Collector: http://otel-collector:4317 (gRPC) or :4318 (HTTP)
#    - Zipkin: http://zipkin:9411
# 3. Set TELEMETRY_EXPORTER_INSECURE=false in production (use TLS)
# 4. Add authentication headers if your backend requires it

# Enable distributed tracing globally
# Default: false (disabled for development)
# Set to true in production to enable
TELEMETRY_ENABLED=false

# Service identification (used in trace metadata)
# This appears in your tracing backend (e.g., Jaeger service list)
TELEMETRY_SERVICE_NAME=residency-scheduler

# Deployment environment tag
# Values: development, staging, production
# Used to filter and label traces by environment
TELEMETRY_ENVIRONMENT=development

# Sampling configuration (0.0 to 1.0, where 1.0 = 100% sampling)
# 1.0 = trace every request (high data volume, useful for debugging)
# 0.1 = trace 10% of requests (better for high-traffic production)
# 0.01 = trace 1% of requests (minimal overhead, sparse data)
TELEMETRY_SAMPLING_RATE=1.0

# Console debug output (useful for local development)
# When enabled, prints spans to console in addition to backend exporter
# Set to false in production to reduce log noise
TELEMETRY_CONSOLE_EXPORT=false

# Exporter backend type
# Supported types:
#   - otlp_grpc (default): OpenTelemetry Protocol via gRPC (port 4317)
#   - otlp_http: OpenTelemetry Protocol via HTTP (port 4318)
#   - jaeger: Jaeger all-in-one exporter (port 6831)
#   - zipkin: Zipkin JSON exporter (port 9411)
TELEMETRY_EXPORTER_TYPE=otlp_grpc

# Exporter endpoint URL
# OTLP gRPC: http://localhost:4317 or https://otel.mycompany.com:4317
# OTLP HTTP: http://localhost:4318 or https://otel.mycompany.com:4318
# Jaeger: http://localhost:6831
# Zipkin: http://localhost:9411/api/v2/spans
#
# Docker Compose setup:
#   http://otel-collector:4317 (or your service name)
# Kubernetes setup:
#   http://otel-collector.monitoring:4317
TELEMETRY_EXPORTER_ENDPOINT=http://localhost:4317

# Use insecure connection (no TLS)
# Development: true (most OTEL collectors listen on insecure by default)
# Production: false (use TLS for security)
TELEMETRY_EXPORTER_INSECURE=true

# Optional: Authentication headers for exporter (JSON format)
# Example: {"Authorization": "Bearer YOUR_API_KEY"}
# Used for:
#   - Datadog: {"DD-API-KEY": "your-api-key"}
#   - New Relic: {"api-key": "your-api-key"}
#   - Custom backends with auth
# Uncomment and set if your backend requires authentication
# TELEMETRY_EXPORTER_HEADERS={"Authorization": "Bearer token"}

# =============================================================================
# Instrumentation Toggles
# =============================================================================
# Controls which components emit traces
# Disabling reduces overhead but loses visibility into those systems

# Enable SQLAlchemy database query tracing
# Traces all SQL queries, execution times, and errors
# Default: true (recommended to keep enabled for debugging slow queries)
TELEMETRY_TRACE_SQLALCHEMY=true

# Enable Redis cache operation tracing
# Traces cache hits/misses, operations, and latency
# Default: true (useful for debugging cache performance)
TELEMETRY_TRACE_REDIS=true

# Enable HTTP client tracing (external API calls)
# Traces outgoing HTTP requests via requests and httpx libraries
# Default: true (useful for debugging external service latency)
TELEMETRY_TRACE_HTTP=true

# -----------------------------------------------------------------------------
# LLM Router Configuration
# -----------------------------------------------------------------------------
# Default LLM provider (ollama, anthropic)
# ollama = local Docker container, anthropic = cloud API
LLM_DEFAULT_PROVIDER=ollama

# Enable fallback to other providers on failure
LLM_ENABLE_FALLBACK=true

# Airgap mode - disable all cloud providers (local only)
# Set to true for offline/secure deployments
LLM_AIRGAP_MODE=false

# Ollama configuration (local LLM)
OLLAMA_URL=http://ollama:11434
OLLAMA_DEFAULT_MODEL=llama3.2
OLLAMA_FAST_MODEL=llama3.2
OLLAMA_TOOL_MODEL=mistral
OLLAMA_TIMEOUT=60.0

# Anthropic API configuration (cloud LLM)
# Leave empty for airgap mode or local-only operation
# Get API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=
ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022

# -----------------------------------------------------------------------------
# Quantum Solver Configuration (optional, experimental)
# -----------------------------------------------------------------------------
# Enable quantum-inspired scheduling solver
# Default: false (use classical solvers: CP-SAT, PuLP, Greedy)
# Set to true to enable quantum-inspired optimization
USE_QUANTUM_SOLVER=false

# Quantum solver backend selection
# Options: "classical" or "quantum"
# - classical: Simulated annealing on classical hardware (no API key needed)
# - quantum: D-Wave quantum annealing hardware (requires DWAVE_API_TOKEN)
# Default: classical
QUANTUM_SOLVER_BACKEND=classical

# D-Wave API token for quantum hardware access
# Get API key from: https://cloud.dwavesys.com/
# Only needed if QUANTUM_SOLVER_BACKEND=quantum
# Leave empty for classical simulation
# DWAVE_API_TOKEN=your_dwave_api_token_here

# NOTE: Quantum solver requires optional dependencies:
# Classical mode: pip install dwave-samplers
# Quantum mode: pip install dwave-system dwave-samplers
# See backend/requirements.txt for details

# -----------------------------------------------------------------------------
# Optional: PostgreSQL Direct Connection (for local development)
# -----------------------------------------------------------------------------
# Full database URL (usually constructed automatically in docker-compose)
# DATABASE_URL=postgresql://scheduler:your_password@localhost:5432/residency_scheduler
