# =============================================================================
# Residency Scheduler - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control
# =============================================================================

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# PostgreSQL password for the scheduler user
# Use a strong, unique password in production
DB_PASSWORD=your_secure_database_password_here

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------
# Secret key for JWT token signing
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
# CRITICAL: Use a unique, random value in production
SECRET_KEY=your_secret_key_here_generate_a_random_64_char_string

# Webhook secret for validating incoming webhook requests
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
# CRITICAL: Use a unique, random value in production (minimum 32 characters)
WEBHOOK_SECRET=your_webhook_secret_here_generate_a_random_64_char_string

# Redis password for authentication
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# For development: defaults to 'dev_only_password' if not set
# For production: MUST be set to a strong random value
REDIS_PASSWORD=your_redis_password_here_generate_a_random_string

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------
# Debug mode - set to false in production
# Enables detailed error messages and hot reload in development
DEBUG=true

# -----------------------------------------------------------------------------
# CORS Configuration
# -----------------------------------------------------------------------------
# Allowed origins for cross-origin requests
# In production, set to your actual frontend domain
# Format: JSON array of strings
CORS_ORIGINS=["http://localhost:3000"]

# -----------------------------------------------------------------------------
# Frontend Configuration
# -----------------------------------------------------------------------------
# Backend API URL that the frontend will use
# In production, use your actual backend domain/IP
NEXT_PUBLIC_API_URL=http://localhost:8000

# -----------------------------------------------------------------------------
# MCP Server Configuration
# -----------------------------------------------------------------------------
# Log level for MCP server (DEBUG, INFO, WARNING, ERROR)
# DEBUG shows all tool invocations and API calls
MCP_LOG_LEVEL=INFO

# =============================================================================
# OpenTelemetry / Distributed Tracing Configuration
# =============================================================================
# Provides distributed tracing across backend services (API, workers, scheduler)
# Useful for debugging latency issues, tracking request flows, and monitoring dependencies
# Default: TELEMETRY_ENABLED=false (disabled for development to avoid performance impact)
#
# PRODUCTION SETUP:
# 1. Enable tracing: TELEMETRY_ENABLED=true
# 2. Set backend endpoint (where traces are exported):
#    - Jaeger all-in-one: http://jaeger:16686 (or your host:port)
#    - OpenTelemetry Collector: http://otel-collector:4317 (gRPC) or :4318 (HTTP)
#    - Zipkin: http://zipkin:9411
# 3. Set TELEMETRY_EXPORTER_INSECURE=false in production (use TLS)
# 4. Add authentication headers if your backend requires it

# Enable distributed tracing globally
# Default: false (disabled for development)
# Set to true in production to enable
TELEMETRY_ENABLED=false

# Service identification (used in trace metadata)
# This appears in your tracing backend (e.g., Jaeger service list)
TELEMETRY_SERVICE_NAME=residency-scheduler

# Deployment environment tag
# Values: development, staging, production
# Used to filter and label traces by environment
TELEMETRY_ENVIRONMENT=development

# Sampling configuration (0.0 to 1.0, where 1.0 = 100% sampling)
# 1.0 = trace every request (high data volume, useful for debugging)
# 0.1 = trace 10% of requests (better for high-traffic production)
# 0.01 = trace 1% of requests (minimal overhead, sparse data)
TELEMETRY_SAMPLING_RATE=1.0

# Console debug output (useful for local development)
# When enabled, prints spans to console in addition to backend exporter
# Set to false in production to reduce log noise
TELEMETRY_CONSOLE_EXPORT=false

# Exporter backend type
# Supported types:
#   - otlp_grpc (default): OpenTelemetry Protocol via gRPC (port 4317)
#   - otlp_http: OpenTelemetry Protocol via HTTP (port 4318)
#   - jaeger: Jaeger all-in-one exporter (port 6831)
#   - zipkin: Zipkin JSON exporter (port 9411)
TELEMETRY_EXPORTER_TYPE=otlp_grpc

# Exporter endpoint URL
# OTLP gRPC: http://localhost:4317 or https://otel.mycompany.com:4317
# OTLP HTTP: http://localhost:4318 or https://otel.mycompany.com:4318
# Jaeger: http://localhost:6831
# Zipkin: http://localhost:9411/api/v2/spans
#
# Docker Compose setup:
#   http://otel-collector:4317 (or your service name)
# Kubernetes setup:
#   http://otel-collector.monitoring:4317
TELEMETRY_EXPORTER_ENDPOINT=http://localhost:4317

# Use insecure connection (no TLS)
# Development: true (most OTEL collectors listen on insecure by default)
# Production: false (use TLS for security)
TELEMETRY_EXPORTER_INSECURE=true

# Optional: Authentication headers for exporter (JSON format)
# Example: {"Authorization": "Bearer YOUR_API_KEY"}
# Used for:
#   - Datadog: {"DD-API-KEY": "your-api-key"}
#   - New Relic: {"api-key": "your-api-key"}
#   - Custom backends with auth
# Uncomment and set if your backend requires authentication
# TELEMETRY_EXPORTER_HEADERS={"Authorization": "Bearer token"}

# =============================================================================
# Instrumentation Toggles
# =============================================================================
# Controls which components emit traces
# Disabling reduces overhead but loses visibility into those systems

# Enable SQLAlchemy database query tracing
# Traces all SQL queries, execution times, and errors
# Default: true (recommended to keep enabled for debugging slow queries)
TELEMETRY_TRACE_SQLALCHEMY=true

# Enable Redis cache operation tracing
# Traces cache hits/misses, operations, and latency
# Default: true (useful for debugging cache performance)
TELEMETRY_TRACE_REDIS=true

# Enable HTTP client tracing (external API calls)
# Traces outgoing HTTP requests via requests and httpx libraries
# Default: true (useful for debugging external service latency)
TELEMETRY_TRACE_HTTP=true

# -----------------------------------------------------------------------------
# LLM Router Configuration
# -----------------------------------------------------------------------------
# Default LLM provider (ollama, anthropic)
# ollama = local Docker container, anthropic = cloud API
LLM_DEFAULT_PROVIDER=ollama

# Enable fallback to other providers on failure
LLM_ENABLE_FALLBACK=true

# Airgap mode - disable all cloud providers (local only)
# Set to true for offline/secure deployments
LLM_AIRGAP_MODE=false

# Ollama configuration (local LLM)
OLLAMA_URL=http://ollama:11434
OLLAMA_DEFAULT_MODEL=llama3.2
OLLAMA_FAST_MODEL=llama3.2
OLLAMA_TOOL_MODEL=mistral
OLLAMA_TIMEOUT=60.0

# Anthropic API configuration (cloud LLM)
# Leave empty for airgap mode or local-only operation
# Get API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=
ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022

# -----------------------------------------------------------------------------
# Optional: PostgreSQL Direct Connection (for local development)
# -----------------------------------------------------------------------------
# Full database URL (usually constructed automatically in docker-compose)
# DATABASE_URL=postgresql://scheduler:your_password@localhost:5432/residency_scheduler
