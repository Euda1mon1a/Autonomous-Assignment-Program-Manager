# Production Docker Compose Configuration
# For development, use: docker-compose -f docker-compose.yml -f docker-compose.dev.yml up

services:
  # PostgreSQL Database with pgvector for AI/ML vector similarity search
  db:
    image: pgvector/pgvector:0.8.1-pg15  # Includes pgvector extension for embeddings
    container_name: residency-scheduler-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: scheduler
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: residency_scheduler
    volumes:
      - postgres_data:/var/lib/postgresql/data
    # Health check - verifies PostgreSQL can accept connections
    # Uses pg_isready to check server status and database connectivity
    # Higher retry count ensures stability during startup
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U scheduler -d residency_scheduler"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - app-network

  # Redis - Message Broker for Celery
  redis:
    image: redis:7.4.5-alpine  # CVE-2025-32023 (hyperloglog RCE), CVE-2025-49844 patched
    container_name: residency-scheduler-redis
    restart: unless-stopped
    # Port removed for production security - Redis only accessible within Docker network
    # For local development, use docker-compose.dev.yml to expose port
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru --requirepass ${REDIS_PASSWORD:-dev_only_password}
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-dev_only_password}
    volumes:
      - redis_data:/data
    # Health check - verifies Redis is accepting commands
    # Uses authenticated PING command to validate server health
    # Quick startup period since Redis initializes fast
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-dev_only_password}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      - app-network

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: residency-scheduler-backend
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://scheduler:${DB_PASSWORD}@db:5432/residency_scheduler
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dev_only_password}
      REDIS_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      SECRET_KEY: ${SECRET_KEY}
      DEBUG: "false"
      CORS_ORIGINS: ${CORS_ORIGINS:-["http://localhost:3000"]}
      RATE_LIMIT_ENABLED: ${RATE_LIMIT_ENABLED:-false}
      # OpenTelemetry Distributed Tracing Configuration
      # For production deployments, enable TELEMETRY_ENABLED and configure OTEL backend
      # Supported backends: Jaeger, Zipkin, DataDog, New Relic, or any OTLP-compatible collector
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-false}
      TELEMETRY_SERVICE_NAME: ${TELEMETRY_SERVICE_NAME:-residency-scheduler}
      TELEMETRY_ENVIRONMENT: ${TELEMETRY_ENVIRONMENT:-production}
      TELEMETRY_SAMPLING_RATE: ${TELEMETRY_SAMPLING_RATE:-1.0}
      TELEMETRY_CONSOLE_EXPORT: ${TELEMETRY_CONSOLE_EXPORT:-false}
      TELEMETRY_EXPORTER_TYPE: ${TELEMETRY_EXPORTER_TYPE:-otlp_grpc}
      TELEMETRY_EXPORTER_ENDPOINT: ${TELEMETRY_EXPORTER_ENDPOINT:-http://localhost:4317}
      TELEMETRY_EXPORTER_INSECURE: ${TELEMETRY_EXPORTER_INSECURE:-false}
      TELEMETRY_TRACE_SQLALCHEMY: ${TELEMETRY_TRACE_SQLALCHEMY:-true}
      TELEMETRY_TRACE_REDIS: ${TELEMETRY_TRACE_REDIS:-true}
      TELEMETRY_TRACE_HTTP: ${TELEMETRY_TRACE_HTTP:-true}
    ports:
      - "8000:8000"
    # Health check - verifies FastAPI server and /health endpoint
    # Uses Python urllib to check HTTP health endpoint response
    # Start period allows time for database migrations
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - app-network

  # Celery Worker
  celery-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: residency-scheduler-celery-worker
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://scheduler:${DB_PASSWORD}@db:5432/residency_scheduler
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dev_only_password}
      REDIS_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      SECRET_KEY: ${SECRET_KEY}
      DEBUG: "false"
      # OpenTelemetry Distributed Tracing for Celery background tasks
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-false}
      TELEMETRY_SERVICE_NAME: ${TELEMETRY_SERVICE_NAME:-residency-scheduler-worker}
      TELEMETRY_ENVIRONMENT: ${TELEMETRY_ENVIRONMENT:-production}
      TELEMETRY_EXPORTER_ENDPOINT: ${TELEMETRY_EXPORTER_ENDPOINT:-http://localhost:4317}
    command: celery -A app.core.celery_app worker --loglevel=info -Q default,resilience,notifications,metrics,exports,security
    networks:
      - app-network
    # Health check - verifies Celery worker is responsive
    # Uses inspect ping to check worker heartbeat
    # Extended start period allows worker to initialize queues
    healthcheck:
      test: ["CMD-SHELL", "celery -A app.core.celery_app inspect ping -d celery@$$HOSTNAME"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Celery Beat Scheduler
  celery-beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: residency-scheduler-celery-beat
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://scheduler:${DB_PASSWORD}@db:5432/residency_scheduler
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dev_only_password}
      REDIS_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      SECRET_KEY: ${SECRET_KEY}
      DEBUG: "false"
      # OpenTelemetry Distributed Tracing for scheduled tasks
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-false}
      TELEMETRY_SERVICE_NAME: ${TELEMETRY_SERVICE_NAME:-residency-scheduler-beat}
      TELEMETRY_ENVIRONMENT: ${TELEMETRY_ENVIRONMENT:-production}
      TELEMETRY_EXPORTER_ENDPOINT: ${TELEMETRY_EXPORTER_ENDPOINT:-http://localhost:4317}
    command: celery -A app.core.celery_app beat --loglevel=info
    # Health check - verifies Celery beat scheduler is running
    # Checks for schedule file presence as indicator of active scheduler
    # Extended start period allows beat to create initial schedule
    healthcheck:
      test: ["CMD-SHELL", "[ -f celerybeat-schedule ] && echo 'healthy' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - app-network

  # Next.js Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: residency-scheduler-frontend
    restart: unless-stopped
    depends_on:
      - backend
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000/api/v1}
    ports:
      - "3000:3000"
    # Health check defined in Dockerfile using node (no wget in image)
    # Dockerfile HEALTHCHECK: node -e "require('http').get(...)"
    networks:
      - app-network

  # MCP Server - Model Context Protocol for AI Assistant Integration
  # Provides 29+ specialized scheduling tools for Claude Code and other AI agents
  # Security model follows Docker MCP Toolkit patterns (resource limits, privilege dropping)
  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: residency-scheduler-mcp
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Transport mode - use HTTP for Docker container (STDIO exits without stdin)
      MCP_TRANSPORT: http
      MCP_HOST: "0.0.0.0"
      MCP_PORT: "8080"
      # API Integration - connects to FastAPI backend (not direct DB)
      API_BASE_URL: http://backend:8000
      # Credentials required - set in .env file
      API_USERNAME: ${API_USERNAME}
      API_PASSWORD: ${API_PASSWORD}
      # Celery for async task management
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-dev_only_password}@redis:6379/0
      # Logging
      LOG_LEVEL: ${MCP_LOG_LEVEL:-INFO}
      # Python path for imports
      PYTHONPATH: /app/src
      # ARMORY - Enable ALL exotic MCP tools (50+ tools across 5 domains)
      # With Claude Code v2.1.7 lazy loading, context tax is eliminated
      ARMORY_DOMAINS: ${ARMORY_DOMAINS:-all}
    # Security constraints (Docker MCP Toolkit pattern)
    # - Resource limits prevent runaway processes
    # - no-new-privileges prevents privilege escalation
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 256M
    # HTTP transport port - bound to localhost only for security
    # This enables parallel MCP connections while preventing network exposure
    # DNS rebinding protection: only accessible from 127.0.0.1
    ports:
      - "127.0.0.1:8080:8080"
    networks:
      - app-network
    # Health check - verifies MCP server module loads successfully
    # Tests Python import to ensure server initialization
    # Quick startup since MCP initializes rapidly
    healthcheck:
      test: ["CMD", "python", "-c", "from scheduler_mcp.server import mcp; print('healthy')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # n8n - Workflow Automation (DISABLED - CVE-2026-21858 Ni8mare, re-enable post-deployment)
  # n8n:
  #   image: n8nio/n8n:1.121.0  # Pin to patched version when re-enabling
  #   container_name: residency-scheduler-n8n
  #   restart: unless-stopped
  #   ports:
  #     - "127.0.0.1:5678:5678"  # Bind to localhost only
  #   environment:
  #     - N8N_BASIC_AUTH_ACTIVE=true
  #     - N8N_BASIC_AUTH_USER=${N8N_USER:-admin}
  #     - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD:-localdev_only}
  #     - N8N_HOST=${N8N_HOST:-localhost}
  #     - N8N_PORT=5678
  #     - N8N_PROTOCOL=http
  #     - WEBHOOK_URL=${N8N_WEBHOOK_URL:-http://localhost:5678}
  #     - GENERIC_TIMEZONE=America/New_York
  #   volumes:
  #     - n8n_data:/home/node/.n8n
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:5678/healthz || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 60s
  #   networks:
  #     - app-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  n8n_data:
    driver: local

networks:
  app-network:
    driver: bridge
